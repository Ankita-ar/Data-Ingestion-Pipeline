# ğŸš€ Data Ingestion & Processing Assignment

This project demonstrates a complete data ingestion pipeline using a large CSV (2+ GB) dataset in a resource-constrained environment like Google Colab. The goal is to evaluate different data reading methods, validate data structure, and efficiently export the processed file.

---

## ğŸ“ Assignment Objectives

- âœ… Read a large CSV file using:
  - Pandas
  - Dask
  - Modin (Ray backend)
- âœ… Compare performance across frameworks
- âœ… Clean column names (remove special characters and whitespaces)
- âœ… Define schema and structure using YAML
- âœ… Validate file structure (column names and count)
- âœ… Export processed data as pipe-separated `.txt.gz` file
- âœ… Generate file summary: rows, columns, and file size

---

## ğŸ“‚ Project Structure

- `assignment_ingestion.ipynb` â€“ Full implementation notebook
- `processed_output.txt.gz` â€“ Final processed file (pipe-separated, gzipped)
- `schema.yaml` â€“ YAML schema (column names, separator, structure)
- `README.md` â€“ This documentation

---

## âš™ï¸ Technologies Used

- Python
- Pandas, Dask, Modin (Ray)
- PyYAML
- Google Colab
- Google Drive (as external storage)

---

## ğŸ“Š Results Summary

- âœ… Efficient file processing in Google Colab (2GB+ file)
- âœ… Output file size: ~200MB (gzip)
- âœ… Compatibility with low-resource environments

---

## ğŸ“ Links

- ğŸ“˜ [Colab Notebook](https://colab.research.google.com/drive/14tHX6rekaBnoHcRrXwJC3VAvIzzbVOVj)

---

## ğŸ™Œ Acknowledgements

This project was completed as part of a practical assignment on data engineering principles and large-scale data handling.
